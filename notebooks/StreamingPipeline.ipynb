{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, col, from_json, when, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Sales_Streaming_Pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "data_bucket_uri = \"data_de2024_a2\"\n",
    "temp_bucket = \"temp_de2024_mh\"\n",
    "project_id = \"core-synthesis-435410-v9\"\n",
    "\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = StructType([\n",
    "    StructField(\"payment_key\", StringType(), True),\n",
    "    StructField(\"coustomer_key\", StringType(), True),\n",
    "    StructField(\"time_key\", StringType(), True),\n",
    "    StructField(\"item_key\", StringType(), True),\n",
    "    StructField(\"store_key\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"unit_price\", FloatType(), True),\n",
    "    StructField(\"total_price\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary dimension Tables\n",
    "itemDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/item_dim.csv\")\n",
    "storeDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/store_dim.csv\")\n",
    "timeDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/time_dim.csv\")\n",
    "transDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/Trans_dim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_key: string (nullable = true)\n",
      " |-- coustomer_key: string (nullable = true)\n",
      " |-- time_key: string (nullable = true)\n",
      " |-- item_key: string (nullable = true)\n",
      " |-- store_key: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- unit_price: float (nullable = true)\n",
      " |-- total_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"subscribe\", \"transaction\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Cast the `value` column to STRING\n",
    "df_string = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse the JSON data\n",
    "df1 = df_string.select(from_json(col(\"value\"), dataSchema).alias(\"parsed_value\"))\n",
    "\n",
    "# Extract fields from the parsed JSON\n",
    "sdf = df1.select(col(\"parsed_value.*\"))\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop eventual duplicates\n",
    "itemDF_cleaned = itemDF.drop(\"unit_price\", \"unit_price\")\n",
    "# Join Fact Table with Item and Store Dimension Tables\n",
    "joinedDF = sdf.join(itemDF_cleaned, \"item_key\").join(storeDF, \"store_key\").join(timeDF, \"time_key\").join(transDF, \"payment_key\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the relevant columns for the dashboard\n",
    "dashboard_df = joinedDF.select(\n",
    "    col(\"coustomer_key\").alias(\"customer\"),  # Renamed to \"customer\"\n",
    "    col(\"store_key\").alias(\"store\"),         # Renamed to \"store\"\n",
    "    col(\"item_name\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"unit_price\"),\n",
    "    col(\"total_price\"),\n",
    "    to_timestamp(col(\"date\"), \"dd-MM-yyyy HH:mm\").alias(\"date\"),\n",
    "    col(\"trans_type\")\n",
    ")\n",
    "\n",
    "# Add a new column \"price_category\"\n",
    "dashboard_df = dashboard_df.withColumn(\n",
    "    \"price_category\",\n",
    "    when(col(\"unit_price\") > 50, \"High\")\n",
    "    .when(col(\"unit_price\") > 20, \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed batch: 0\n",
      "Pushed batch: 1\n",
      "Pushed batch: 2\n",
      "Pushed batch: 3\n",
      "Pushed batch: 4\n",
      "Pushed batch: 5\n",
      "Pushed batch: 6\n",
      "Pushed batch: 7\n",
      "Pushed batch: 8\n",
      "Pushed batch: 9\n",
      "Pushed batch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stoped the streaming query and the spark context\n"
     ]
    }
   ],
   "source": [
    "def write_results(df, batch_id):\n",
    "    print('Pushed batch:', batch_id)\n",
    "    # df.show()\n",
    "    df \\\n",
    "      .write.format('bigquery') \\\n",
    "      .option('table', f'{project_id}.a2.transaction') \\\n",
    "      .mode(\"append\") \\\n",
    "      .save()\n",
    "\n",
    "query = dashboard_df \\\n",
    "    .writeStream.outputMode(\"append\") \\\n",
    "    .foreachBatch(write_results) \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "except:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Unexpected error\")\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactionCounts = sdf.groupBy(\"store_key\").count()\n",
    "\n",
    "# query = transactionCounts \\\n",
    "#               .select(concat(col(\"store_key\"), lit(\" \"), col(\"count\")).alias(\"value\")) \\\n",
    "#               .writeStream \\\n",
    "#               .format(\"kafka\") \\\n",
    "#               .option(\"kafka.bootstrap.servers\", \"kafka1:9093\").option(\"failOnDataLoss\", \"false\") \\\n",
    "#               .option(\"checkpointLocation\", \"/home/jovyan/checkpoint/store_transaction_count\")\\\n",
    "#               .option(\"topic\", \"store_transaction_count\") \\\n",
    "#               .outputMode(\"complete\") \\\n",
    "#               .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
