{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, col, from_json, when, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Sales_Streaming_Pipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "data_bucket_uri = \"data_de2024_a2\"\n",
    "temp_bucket = \"temp_de2024_mh\"\n",
    "project_id = \"core-synthesis-435410-v9\"\n",
    "\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = StructType([\n",
    "    StructField(\"payment_key\", StringType(), True),\n",
    "    StructField(\"coustomer_key\", StringType(), True),\n",
    "    StructField(\"time_key\", StringType(), True),\n",
    "    StructField(\"item_key\", StringType(), True),\n",
    "    StructField(\"store_key\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"unit_price\", FloatType(), True),\n",
    "    StructField(\"total_price\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary dimension Tables\n",
    "itemDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/item_dim.csv\")\n",
    "storeDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/store_dim.csv\")\n",
    "timeDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/time_dim.csv\")\n",
    "transDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"gs://{data_bucket_uri}/Trans_dim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_key: string (nullable = true)\n",
      " |-- coustomer_key: string (nullable = true)\n",
      " |-- time_key: string (nullable = true)\n",
      " |-- item_key: string (nullable = true)\n",
      " |-- store_key: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- unit_price: float (nullable = true)\n",
      " |-- total_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9093\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"subscribe\", \"transaction\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Cast the `value` column to STRING\n",
    "df_string = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse the JSON data\n",
    "df1 = df_string.select(from_json(col(\"value\"), dataSchema).alias(\"parsed_value\"))\n",
    "\n",
    "# Extract fields from the parsed JSON\n",
    "sdf = df1.select(col(\"parsed_value.*\"))\n",
    "\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop eventual duplicates\n",
    "itemDF_cleaned = itemDF.drop(\"unit_price\", \"unit_price\")\n",
    "# Join Fact Table with Item and Store Dimension Tables\n",
    "joinedDF = sdf.join(itemDF_cleaned, \"item_key\").join(storeDF, \"store_key\").join(timeDF, \"time_key\").join(transDF, \"payment_key\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the relevant columns for the dashboard\n",
    "dashboard_df = joinedDF.select(\n",
    "    col(\"coustomer_key\").alias(\"customer\"),  # Renamed to \"customer\"\n",
    "    col(\"store_key\").alias(\"store\"),         # Renamed to \"store\"\n",
    "    col(\"item_name\"),\n",
    "    col(\"quantity\"),\n",
    "    col(\"unit_price\"),\n",
    "    col(\"total_price\"),\n",
    "    to_timestamp(col(\"date\"), \"dd-MM-yyyy HH:mm\").alias(\"timestamp\"),\n",
    "    col(\"trans_type\")\n",
    ")\n",
    "\n",
    "# Add a new column \"price_category\"\n",
    "dashboard_df = dashboard_df.withColumn(\n",
    "    \"price_category\",\n",
    "    when(col(\"unit_price\") > 50, \"High\")\n",
    "    .when(col(\"unit_price\") > 20, \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed batch: '0' ~ Consisting of 0 rows.\n",
      "Pushed batch: '1' ~ Consisting of 5 rows.\n",
      "Pushed batch: '2' ~ Consisting of 65 rows.\n",
      "Pushed batch: '3' ~ Consisting of 64 rows.\n",
      "Pushed batch: '4' ~ Consisting of 50 rows.\n",
      "Pushed batch: '5' ~ Consisting of 50 rows.\n",
      "Pushed batch: '6' ~ Consisting of 48 rows.\n",
      "Pushed batch: '7' ~ Consisting of 45 rows.\n",
      "Pushed batch: '8' ~ Consisting of 48 rows.\n",
      "Pushed batch: '9' ~ Consisting of 57 rows.\n",
      "Pushed batch: '10' ~ Consisting of 62 rows.\n",
      "Pushed batch: '11' ~ Consisting of 57 rows.\n",
      "Pushed batch: '12' ~ Consisting of 53 rows.\n",
      "Pushed batch: '13' ~ Consisting of 52 rows.\n",
      "Pushed batch: '14' ~ Consisting of 50 rows.\n",
      "Pushed batch: '15' ~ Consisting of 54 rows.\n",
      "Pushed batch: '16' ~ Consisting of 50 rows.\n",
      "Pushed batch: '17' ~ Consisting of 53 rows.\n",
      "Pushed batch: '18' ~ Consisting of 49 rows.\n",
      "Pushed batch: '19' ~ Consisting of 50 rows.\n",
      "Pushed batch: '20' ~ Consisting of 46 rows.\n",
      "Pushed batch: '21' ~ Consisting of 55 rows.\n",
      "Pushed batch: '22' ~ Consisting of 43 rows.\n",
      "Pushed batch: '23' ~ Consisting of 53 rows.\n",
      "Pushed batch: '24' ~ Consisting of 61 rows.\n",
      "Pushed batch: '25' ~ Consisting of 56 rows.\n",
      "Pushed batch: '26' ~ Consisting of 53 rows.\n",
      "Pushed batch: '27' ~ Consisting of 50 rows.\n",
      "Pushed batch: '28' ~ Consisting of 46 rows.\n",
      "Pushed batch: '29' ~ Consisting of 60 rows.\n",
      "Pushed batch: '30' ~ Consisting of 45 rows.\n",
      "Pushed batch: '31' ~ Consisting of 45 rows.\n",
      "Pushed batch: '32' ~ Consisting of 47 rows.\n",
      "Pushed batch: '33' ~ Consisting of 72 rows.\n",
      "Pushed batch: '34' ~ Consisting of 44 rows.\n",
      "Pushed batch: '35' ~ Consisting of 58 rows.\n",
      "Pushed batch: '36' ~ Consisting of 52 rows.\n",
      "Pushed batch: '37' ~ Consisting of 44 rows.\n",
      "Pushed batch: '38' ~ Consisting of 48 rows.\n",
      "Pushed batch: '39' ~ Consisting of 44 rows.\n",
      "Pushed batch: '40' ~ Consisting of 53 rows.\n",
      "Pushed batch: '41' ~ Consisting of 46 rows.\n",
      "Pushed batch: '42' ~ Consisting of 48 rows.\n",
      "Pushed batch: '43' ~ Consisting of 57 rows.\n",
      "Pushed batch: '44' ~ Consisting of 47 rows.\n",
      "Pushed batch: '45' ~ Consisting of 53 rows.\n",
      "Pushed batch: '46' ~ Consisting of 52 rows.\n",
      "Pushed batch: '47' ~ Consisting of 63 rows.\n",
      "Pushed batch: '48' ~ Consisting of 42 rows.\n",
      "Pushed batch: '49' ~ Consisting of 48 rows.\n",
      "Pushed batch: '50' ~ Consisting of 44 rows.\n",
      "Pushed batch: '51' ~ Consisting of 46 rows.\n",
      "Pushed batch: '52' ~ Consisting of 54 rows.\n",
      "Pushed batch: '53' ~ Consisting of 45 rows.\n",
      "Pushed batch: '54' ~ Consisting of 62 rows.\n",
      "Pushed batch: '55' ~ Consisting of 45 rows.\n",
      "Pushed batch: '56' ~ Consisting of 40 rows.\n",
      "Pushed batch: '57' ~ Consisting of 52 rows.\n",
      "Pushed batch: '58' ~ Consisting of 58 rows.\n",
      "Pushed batch: '59' ~ Consisting of 58 rows.\n",
      "Pushed batch: '60' ~ Consisting of 50 rows.\n",
      "Pushed batch: '61' ~ Consisting of 61 rows.\n",
      "Pushed batch: '62' ~ Consisting of 53 rows.\n",
      "Pushed batch: '63' ~ Consisting of 53 rows.\n",
      "Pushed batch: '64' ~ Consisting of 7 rows.\n"
     ]
    }
   ],
   "source": [
    "query = dashboard_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, batch_id: (\n",
    "        print(f\"Pushed batch: '{batch_id}'\", f\"~ Consisting of {df.count()} rows.\"),\n",
    "        df.write\n",
    "          .format('bigquery')\n",
    "          .option('table', f'{project_id}.a2.transactions')\n",
    "          .mode(\"append\")\n",
    "          .save()\n",
    "    )) \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "except:\n",
    "    query.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Unexpected error\")\n",
    "    print(\"Stoped the streaming query and the spark context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactionCounts = sdf.groupBy(\"store_key\").count()\n",
    "\n",
    "# query = transactionCounts \\\n",
    "#               .select(concat(col(\"store_key\"), lit(\" \"), col(\"count\")).alias(\"value\")) \\\n",
    "#               .writeStream \\\n",
    "#               .format(\"kafka\") \\\n",
    "#               .option(\"kafka.bootstrap.servers\", \"kafka1:9093\").option(\"failOnDataLoss\", \"false\") \\\n",
    "#               .option(\"checkpointLocation\", \"/home/jovyan/checkpoint/store_transaction_count\")\\\n",
    "#               .option(\"topic\", \"store_transaction_count\") \\\n",
    "#               .outputMode(\"complete\") \\\n",
    "#               .start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
