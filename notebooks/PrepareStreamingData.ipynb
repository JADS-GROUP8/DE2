{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset: ['customer_dim.csv', 'fact_table.csv', 'item_dim.csv', 'merged_ecommerce_data.csv', 'store_dim.csv', 'time_dim.csv', 'Trans_dim.csv']\n",
      "DataFrames loaded: ['customer_dim.csv', 'fact_table.csv', 'item_dim.csv', 'merged_ecommerce_data.csv', 'store_dim.csv', 'time_dim.csv', 'Trans_dim.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"mmohaiminulislam/ecommerce-data-analysis\")\n",
    "\n",
    "# List all files in the dataset directory\n",
    "files = os.listdir(path)\n",
    "print(\"Files in dataset:\", files)\n",
    "\n",
    "# Load each CSV file into a DataFrame with specified encoding\n",
    "dfs = {file: pd.read_csv(os.path.join(path, file), encoding='ISO-8859-1') for file in files if file.endswith('.csv')}\n",
    "\n",
    "# Print the keys of the dictionary to show the different DataFrames\n",
    "print(\"DataFrames loaded:\", list(dfs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count    Dtype  \n",
      "---  ------         --------------    -----  \n",
      " 0   payment_key    1000000 non-null  object \n",
      " 1   coustomer_key  1000000 non-null  object \n",
      " 2   time_key       1000000 non-null  object \n",
      " 3   item_key       1000000 non-null  object \n",
      " 4   store_key      1000000 non-null  object \n",
      " 5   quantity       1000000 non-null  int64  \n",
      " 6   unit           996277 non-null   object \n",
      " 7   unit_price     1000000 non-null  float64\n",
      " 8   total_price    1000000 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 68.7+ MB\n"
     ]
    }
   ],
   "source": [
    "dfs.get('fact_table.csv').info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve missing values in the 'unit' column\n",
    "fact_table = dfs.get('fact_table.csv')\n",
    "fact_table['unit'] = fact_table['unit'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the fact_table DataFrame to a list of dictionaries\n",
    "fact_table_data = fact_table.to_dict(orient='records')\n",
    "data_splits = 20\n",
    "\n",
    "# Determine the number of records per part (split into 10 parts)\n",
    "total_records = len(fact_table_data)\n",
    "records_per_part = math.ceil(total_records / data_splits)\n",
    "\n",
    "# Split the data into 10 parts and save each as a valid JSON file\n",
    "for i in range(data_splits):\n",
    "    part_data = fact_table_data[i * records_per_part:(i + 1) * records_per_part]\n",
    "    with open(f'../data/fact_table_part_{i + 1}.json', 'w') as part_file:\n",
    "        json.dump(part_data, part_file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
